# -*- coding: utf-8 -*-
"""make_persian_biwords_dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nr8m3DBclXVzdZftqCW0nh78o-9v3CjL
"""

from google.colab import drive
drive.mount('/content/gdrive')

with open('/content/gdrive/My Drive/Duolingo/output.txt', 'r') as f:
  str_wiki = f.read()

print(str_wiki)

import re

def remove_punctuation(text):
    # Remove punctuation marks and special characters
    cleaned_text = re.sub(r'[^\w\s]', '', text)
    return cleaned_text

def remove_numbers(text):
    # Remove digits
    cleaned_text = re.sub(r'\d', '', text)
    return cleaned_text

def create_biwords(biwords, tokens):
    """
    Creates biwords from a list of tokens.
    Returns a list of biwords.
    """
    for i in range(len(tokens) - 1):
        biword = tokens[i] + " " + tokens[i + 1]
        biwords.append(biword)
    return biwords

def create_biword_counts(biwords):
    """
    Creates a dictionary of biword counts.
    Returns a dictionary where keys are biwords and values are their counts.
    """
    biword_counts = {}
    for biword in biwords:
        if biword in biword_counts:
            biword_counts[biword] += 1
        else:
            biword_counts[biword] = 1
    return biword_counts

# Example input: Replace this with your actual string of sentences

cleaned_persian = remove_numbers(remove_punctuation(str_wiki))

# Tokenize the sentences
sentences = cleaned_persian.split('.')


# Create biwords
biwords = []
for sentence in sentences:
    tokens = sentence.split()
    tokens.insert(0, '$')
    tokens.append('$')
    biwords = create_biwords(biwords, tokens)

# Create a dictionary of biword counts
biword_counts = create_biword_counts(biwords)


try:
    with open("biword_counts.txt", "wt") as file:
        file.write(str(biword_counts))
    print("Biword dictionary saved to biwords.txt")
except IOError:
    print("Unable to write to file")